{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08cfc115-36f6-43e0-bc01-ca3eeafa7bda",
   "metadata": {},
   "source": [
    "# Tarefa 02: O Algoritmo Random Forest (Floresta Aleatória)\n",
    "\n",
    "## 1. Passo a Passo do Algoritmo Random Forest (RF)\n",
    "\n",
    "O funcionamento da **Random Forest** é um processo iterativo que visa aumentar a diversidade entre os modelos. Os estágios principais são:\n",
    "\n",
    "* **Amostragem Bootstrap:** Assim como no Bagging, o algoritmo cria múltiplos subconjuntos de dados sorteando linhas aleatoriamente com reposição.\n",
    "* **Seleção Aleatória de Recursos (Feature Selection):** Em cada nó de cada árvore, em vez de buscar a melhor divisão entre todas as variáveis, o algoritmo sorteia um subconjunto aleatório de colunas (features).\n",
    "* **Criação das Árvores (Modelagem):** Uma Árvore de Decisão é treinada para cada amostra. Devido à restrição de variáveis do passo anterior, cada árvore \"enxerga\" o problema de um ângulo diferente.\n",
    "* **Crescimento Máximo:** Geralmente, as árvores na Random Forest são deixadas crescer profundamente, permitindo que cada uma capture padrões específicos da sua amostra.\n",
    "* **Votação/Média (Agregação):** As previsões individuais de todas as árvores são combinadas para gerar o resultado final (Votação para classificação ou Média para regressão).\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Explicação Conceitual\n",
    "\n",
    "A **Random Forest** pode ser vista como uma evolução técnica do Bagging. Ela utiliza múltiplas árvores de decisão, mas adiciona uma camada extra de aleatoriedade na seleção de variáveis (*features*). \n",
    "\n",
    "Como cada árvore analisa dados e sintomas distintos, o diagnóstico final torna-se muito mais sólido e diversificado. Essa abordagem impede que o modelo se torne \"viciado\" em uma única variável predominante que, embora pareça óbvia, poderia levar a conclusões incorretas se analisada isoladamente. Na prática, a Random Forest descorrelaciona as árvores, garantindo que o erro de uma não seja repetido por todas as outras.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Diferença Fundamental: Bagging vs. Random Forest\n",
    "\n",
    "| Característica | Bagging | Random Forest |\n",
    "| :--- | :--- | :--- |\n",
    "| **Amostragem de Linhas** | Bootstrap (com reposição) | Bootstrap (com reposição) |\n",
    "| **Amostragem de Colunas** | Considera todas as variáveis | Considera um subconjunto aleatório |\n",
    "| **Independência** | Árvores podem ser parecidas | Árvores são forçadas a ser diferentes |\n",
    "| **Aplicação Principal** | Redução de Variância | Redução de Variância e Descorrelação |\n",
    "\n",
    "------------\n",
    "\n",
    "## 4. Implementação em Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "319edbed-dc3f-429f-8db8-83b3b8ad3765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2deba4d-a36e-42da-a3d8-08a935757ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relatório de Classificação - Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.60      0.55        72\n",
      "           1       0.57      0.49      0.52        78\n",
      "\n",
      "    accuracy                           0.54       150\n",
      "   macro avg       0.54      0.54      0.54       150\n",
      "weighted avg       0.54      0.54      0.54       150\n",
      "\n",
      "\n",
      "Variáveis que mais influenciaram a floresta:\n",
      "PETR4.SA    0.503651\n",
      "ITUB4.SA    0.496349\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Preparação dos dados\n",
    "df_fechamento = pd.read_csv('dados_ativos_fechamento.csv', index_col=0, parse_dates=True)\n",
    "df_ret = df_fechamento.pct_change().dropna()\n",
    "\n",
    "# Alvo: VALE3 subiu hoje? (1 para sim, 0 para não)\n",
    "X = df_ret.drop(columns=['VALE3.SA']) # Usamos PETR e ITUB para prever VALE\n",
    "y = (df_ret['VALE3.SA'] > 0).astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Implementando a Random Forest\n",
    "# Aqui o RandomForestClassifier já faz o Bootstrap, Feature Selection e Agregação\n",
    "modelo_rf = RandomForestClassifier(\n",
    "    n_estimators=100,      # Modelagem com 100 Decision Trees\n",
    "    max_features='sqrt',   # Feature Selection: raiz quadrada das colunas\n",
    "    bootstrap=True,        # Ativa o Bootstrap\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Treinamento\n",
    "modelo_rf.fit(X_train, y_train)\n",
    "\n",
    "# Resultados\n",
    "previsoes = modelo_rf.predict(X_test)\n",
    "print(\"Relatório de Classificação - Random Forest:\")\n",
    "print(classification_report(y_test, previsoes))\n",
    "\n",
    "# Importância das Variáveis\n",
    "importancias = pd.Series(modelo_rf.feature_importances_, index=X.columns)\n",
    "print(\"\\nVariáveis que mais influenciaram a floresta:\")\n",
    "print(importancias.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46a0576-251e-4f8d-a6ea-822cbce4ad40",
   "metadata": {},
   "source": [
    "---------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
